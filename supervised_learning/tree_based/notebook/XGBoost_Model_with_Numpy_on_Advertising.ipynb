{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost Numpy with Advertising\n",
        "- **Task**: Regression\n",
        "- **Data**: Advertising Dataset\n",
        "- **Model**: XGBoost\n",
        "- **Criterion**: Entropy"
      ],
      "metadata": {
        "id": "nVpRcWMzcuNU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6As6Bg3cbVc",
        "outputId": "19225782-23c2-4952-9793-c4223f8d44fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.6 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.6 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q watermark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext watermark\n",
        "%watermark -a 'Nguyen Van Sy Thinh' -v -p torch"
      ],
      "metadata": {
        "id": "YsBItmQ8dC45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81943ea2-e6ba-4efa-b2dc-8cd871915f4a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Author: Nguyen Van Sy Thinh\n",
            "\n",
            "Python implementation: CPython\n",
            "Python version       : 3.10.12\n",
            "IPython version      : 7.34.0\n",
            "\n",
            "torch: 2.3.0+cu121\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Import Library"
      ],
      "metadata": {
        "id": "a0aCKyKOdDFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "import math"
      ],
      "metadata": {
        "id": "iTgxLmn_dEJS"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data"
      ],
      "metadata": {
        "id": "1Z2b3kNUdGPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Advertising Dataset"
      ],
      "metadata": {
        "id": "qlns7LFTdS-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/advertising.csv\")\n",
        "\n",
        "#X,y\n",
        "X = data.iloc[:,:-1]\n",
        "y = data.iloc[:,-1]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=100)"
      ],
      "metadata": {
        "id": "uwTS_KHldHTi"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model"
      ],
      "metadata": {
        "id": "dMOTCu-MdHb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. Utils"
      ],
      "metadata": {
        "id": "5-OzEgs4lOG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SquaredErrorObjective():\n",
        "    def loss(self, y, pred): return np.mean((y - pred)**2)\n",
        "    def gradient(self, y, pred): return pred - y\n",
        "    def hessian(self, y, pred): return np.ones(len(y))"
      ],
      "metadata": {
        "id": "2THBn6gjlO2L"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2. GradientBoost Model"
      ],
      "metadata": {
        "id": "MIh1BUojdJkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "class XGBoostRegressor():\n",
        "    '''XGBoost from Scratch\n",
        "    '''\n",
        "\n",
        "    def __init__(self, params, random_seed=None):\n",
        "        self.params = defaultdict(lambda: None, params)\n",
        "        self.subsample = self.params['subsample'] \\\n",
        "            if self.params['subsample'] else 1.0\n",
        "        self.learning_rate = self.params['learning_rate'] \\\n",
        "            if self.params['learning_rate'] else 0.3\n",
        "        self.base_prediction = self.params['base_score'] \\\n",
        "            if self.params['base_score'] else 0.5\n",
        "        self.max_depth = self.params['max_depth'] \\\n",
        "            if self.params['max_depth'] else 5\n",
        "        self.rng = np.random.default_rng(seed=random_seed)\n",
        "\n",
        "    def fit(self, X, y, objective, num_boost_round, verbose=False):\n",
        "        current_predictions = self.base_prediction * np.ones(shape=y.shape)\n",
        "        self.boosters = []\n",
        "        for i in range(num_boost_round):\n",
        "            gradients = objective.gradient(y, current_predictions)\n",
        "            hessians = objective.hessian(y, current_predictions)\n",
        "            sample_idxs = None if self.subsample == 1.0 \\\n",
        "                else self.rng.choice(len(y),\n",
        "                                     size=math.floor(self.subsample*len(y)),\n",
        "                                     replace=False)\n",
        "            booster = TreeBooster(X, gradients, hessians,\n",
        "                                  self.params, self.max_depth, sample_idxs)\n",
        "            current_predictions += self.learning_rate * booster.predict(X)\n",
        "            self.boosters.append(booster)\n",
        "            if verbose:\n",
        "                print(f'[{i}] train loss = {objective.loss(y, current_predictions)}')\n",
        "\n",
        "    def predict(self, X):\n",
        "        return (self.base_prediction + self.learning_rate\n",
        "                * np.sum([booster.predict(X) for booster in self.boosters], axis=0))\n",
        "\n",
        "class TreeBooster():\n",
        "\n",
        "    def __init__(self, X, g, h, params, max_depth, idxs=None):\n",
        "        self.params = params\n",
        "        self.max_depth = max_depth\n",
        "        assert self.max_depth >= 0, 'max_depth must be nonnegative'\n",
        "        self.min_child_weight = params['min_child_weight'] \\\n",
        "            if params['min_child_weight'] else 1.0\n",
        "        self.reg_lambda = params['reg_lambda'] if params['reg_lambda'] else 1.0\n",
        "        self.gamma = params['gamma'] if params['gamma'] else 0.0\n",
        "        self.colsample_bynode = params['colsample_bynode'] \\\n",
        "            if params['colsample_bynode'] else 1.0\n",
        "        if isinstance(g, pd.Series): g = g.values\n",
        "        if isinstance(h, pd.Series): h = h.values\n",
        "        if idxs is None: idxs = np.arange(len(g))\n",
        "        self.X, self.g, self.h, self.idxs = X, g, h, idxs\n",
        "        self.n, self.c = len(idxs), X.shape[1]\n",
        "        self.value = -g[idxs].sum() / (h[idxs].sum() + self.reg_lambda) # Eq (5)\n",
        "        self.best_score_so_far = 0.\n",
        "        if self.max_depth > 0:\n",
        "            self._maybe_insert_child_nodes()\n",
        "\n",
        "    def _maybe_insert_child_nodes(self):\n",
        "        for i in range(self.c): self._find_better_split(i)\n",
        "        if self.is_leaf: return\n",
        "        x = self.X.values[self.idxs,self.split_feature_idx]\n",
        "        left_idx = np.nonzero(x <= self.threshold)[0]\n",
        "        right_idx = np.nonzero(x > self.threshold)[0]\n",
        "        self.left = TreeBooster(self.X, self.g, self.h, self.params,\n",
        "                                self.max_depth - 1, self.idxs[left_idx])\n",
        "        self.right = TreeBooster(self.X, self.g, self.h, self.params,\n",
        "                                 self.max_depth - 1, self.idxs[right_idx])\n",
        "\n",
        "    @property\n",
        "    def is_leaf(self): return self.best_score_so_far == 0.\n",
        "\n",
        "    def _find_better_split(self, feature_idx):\n",
        "        x = self.X.values[self.idxs, feature_idx]\n",
        "        g, h = self.g[self.idxs], self.h[self.idxs]\n",
        "        sort_idx = np.argsort(x)\n",
        "        sort_g, sort_h, sort_x = g[sort_idx], h[sort_idx], x[sort_idx]\n",
        "        sum_g, sum_h = g.sum(), h.sum()\n",
        "        sum_g_right, sum_h_right = sum_g, sum_h\n",
        "        sum_g_left, sum_h_left = 0., 0.\n",
        "\n",
        "        for i in range(0, self.n - 1):\n",
        "            g_i, h_i, x_i, x_i_next = sort_g[i], sort_h[i], sort_x[i], sort_x[i + 1]\n",
        "            sum_g_left += g_i; sum_g_right -= g_i\n",
        "            sum_h_left += h_i; sum_h_right -= h_i\n",
        "            if sum_h_left < self.min_child_weight or x_i == x_i_next:continue\n",
        "            if sum_h_right < self.min_child_weight: break\n",
        "\n",
        "            gain = 0.5 * ((sum_g_left**2 / (sum_h_left + self.reg_lambda))\n",
        "                            + (sum_g_right**2 / (sum_h_right + self.reg_lambda))\n",
        "                            - (sum_g**2 / (sum_h + self.reg_lambda))\n",
        "                            ) - self.gamma/2 # Eq(7) in the xgboost paper\n",
        "            if gain > self.best_score_so_far:\n",
        "                self.split_feature_idx = feature_idx\n",
        "                self.best_score_so_far = gain\n",
        "                self.threshold = (x_i + x_i_next) / 2\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._predict_row(row) for i, row in X.iterrows()])\n",
        "\n",
        "    def _predict_row(self, row):\n",
        "        if self.is_leaf:\n",
        "            return self.value\n",
        "        child = self.left if row[self.split_feature_idx] <= self.threshold \\\n",
        "            else self.right\n",
        "        return child._predict_row(row)"
      ],
      "metadata": {
        "id": "TYUYdYFHdIZ1"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. Hyperparameters"
      ],
      "metadata": {
        "id": "EUZ3bvyDlA3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    'learning_rate': 0.1,\n",
        "    'max_depth': 5,\n",
        "    'subsample': 0.8,\n",
        "    'reg_lambda': 1.5,\n",
        "    'gamma': 0.0,\n",
        "    'min_child_weight': 25,\n",
        "    'base_score': 0.0,\n",
        "    'tree_method': 'exact',\n",
        "}\n",
        "num_boost_round = 50"
      ],
      "metadata": {
        "id": "aDiQpECflEt7"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. Init model"
      ],
      "metadata": {
        "id": "dL2sZ8uzlE_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train the from-scratch XGBoost model\n",
        "model_scratch = XGBoostRegressor(params, random_seed=42)\n",
        "model_scratch.fit(X_train, y_train, SquaredErrorObjective(), num_boost_round)"
      ],
      "metadata": {
        "id": "BGf3YnFod35a"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Inference"
      ],
      "metadata": {
        "id": "xeDzp7xFd5Gl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. Scratch Model"
      ],
      "metadata": {
        "id": "eNT_xAA2mRSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_scratch = model_scratch.predict(X_test)\n",
        "print(f'Our implementation: {SquaredErrorObjective().loss(y_test, pred_scratch)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YPAADiVmSti",
        "outputId": "8aae6559-8581-4eab-bd06-53f1c128f0db"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our implementation: 1.7017064684759806\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2. Library Model"
      ],
      "metadata": {
        "id": "GAf8O1ZpmTyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "model = XGBRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "pred_scratch = model.predict(X_test)\n",
        "print(f'XGBRegressor Library: {SquaredErrorObjective().loss(y_test, pred_scratch)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfRPi8IImTlo",
        "outputId": "95059ed3-dd54-4df2-ec3c-13def6ec08ed"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBRegressor Library: 1.0949672299526045\n"
          ]
        }
      ]
    }
  ]
}